Infrastructure as Code(IaC) is the process of provisioning,configuring and managing infra like vm,db,n/w,lb etc through code and automation rather than manual process. 
There are 2 common approaches:
1. Declarative IaC: describes desired end state of infra and IaC tool figure out how to achieve it. Focus on what infra look like rather than specifying steps to get there. (ex: Terraform,CloudFormation, Azure Resource Manager template) 

2. Imperative IaC: Involves specifying step by step instruction to achive end state. but commands need to be executed in right order. (ex: Ansible,chef,puppet)

IaC tools are classified in 2 types:
1. Infra provisioning tools: Terraform,CloudFormation, Azure (ARM) template, Google cloud deployment Manager etc
2. Configuration management tool: Ansible, chef,puppet etc

These 2 categories are mutually exclusive , as most configuration management tools can do some degree of provisioning and most provisioning tools can do some degree of configuration management. 

Mutable vs Immutable infra:
Mutable infra: Infra can be modified or updated once its provisioned makes version tracking much more difficult.
Immutable infra: cannot be modified once provisioned. Involves provisioning new infra if existing infra must be changed. it eliminates configuration drift(changes to infra occurs over time that are not recorded), easy to version control. 

Idempotence: No matter how many times you run IaC, your end up with same end state. If the desired end state is already achieved then system will not change any configuration even after running the IaC code again. 


---------Terraform--------------
Terraform is HashiCorp's infra as tool. Lets you define infra in human readable,declarative configuration files and manage your infra lifecycle. written in HCL. Terraform use the cloud provider APIs to provision infra.(cloud agnnostic)

--providers--
Terraform relies on plugins called providers to interact with cloud providers like aws,azure,gcp,saas providers etc. Terraform config must declare whcih provider they want so that terraform can install and use them. Additionally, some providers require additional config like credentials,versions,endpoint URL's,region details before they can be used.  
 
Terraform workflow:
Write ---------------------------------> Plan------------------------------------- --> Apply
(define infra in config files. 		Dry run to preview changes before applying.	On Approval terraform provision infra from config files. 


Terraform structure:
project-root/
|-- main.tf			--> contains main config of infra like definition of resouces(ec2,vpc,db etc)
|-- variables.tf		--> declare input variables for your terraform. parametrized your code. 
|-- outputs.tf			--> display info about deployed infra
|-- terraform.tfvars		--> used to set specific values for the variables declared in variables.tf
|-- providers.tf		--> define providers and their config
|-- modules/			--> folder contains sub-modules for different parts of your infrastructure. each module can have its own .tf files as above. 
|   |-- networking/
|   |   |-- main.tf
|   |   |-- variables.tf
|   |   |-- outputs.tf
|   |-- compute/
|       |-- main.tf
|       |-- variables.tf
|       |-- outputs.tf
|-- .terraform/			--> directory contains the local Terraform state and other Terraform-related files.
|-- terraform.tfstate		--> contains the current state of your infrastructure. It is managed by Terraform and should not be modified manually.
|-- terraform.tfstate.backup	--> Backup of the state file and is automatically created by Terraform.
	

1. Terraform block: Specifies the required providers that terraform should download before exicuting the terraform script. block contains source(from where terraform should download provider) and its version

terraform {				--> If we miss to include terraform block then based on the provider terraform install required provider
  required_providers{
   aws{
       source =  "hashicorp/aws"
       version = "5.22.0"
     }
   }
}

2. Provider block: Specifies cloud provider and credentials required to connect to providers service. includes name,version,access key an secret key(for aws)
provider "aws" {
  region = "ap-south-1"
  access_key = "my-access-key"
  secret_key = "my_secret_key"
 }


3. Resource Block: Resources are the most imp element in terraform language. each resource block describes one or more infra object like ec2, db, lb etc
resource "aws_instance" "web" {
 ami = "ami-123321"
 instance_type = "t2.micro"
 tags = {
  Env = "PROD"
  }
}


4. Variable Block: defines input variable to be used in terraform config. includes description, type and default value (if not set then prompt for input)
 variable "bucket_name" {
   description = " Name fo s3 bucket"
   type = string
   default = "my-terraform-bucket"
 }

5. Output Block: makes information about infra available on CLI
resource "aws_instance" "web"{
 ami = "ami-1233221"
 instance_type = "t2.micro"
 }

output "ec2_ip" {
 value = aws_instance.web.public_ip
}


6. Data block: Its similar to resource black however it does not create or manages the resource rather it fetches info about existing infra objects. 
data "aws_instance" "my_ec2" {
  depends_on = [aws_instance.web]
  id = aws_instance.web.id
 }
  
7. Module block:
8. Local Values block: 


Terraform state: its JSON file that stores info about resources that terraform manages as well as their current state and dependancies. This file keeps track of remote state of infra created by config and maps them to real world resource. Terraform uses state file to decide changes that need to be made when new config is applied. 

when you execute terraform apply, a new terraform.tfstate is created and previous state is written into backup file terraform.tfstate.backup. the state file can be kept locally where terraform is running or remotely using remote backend (s3, hashicorp consul, azure storage account etc)


-------------Terraform language---------------
Terraform language's main purpose is to declare the resources,which are infa objects. 
<BLOCK TYPE> "<BLOCK LABEL>" "<BLOCK LABEL>" {
#Block body
<identifier> = <expression> #argument (key,value)
}

example: 
resource "aws_vpc" "main"{
cidr_block = var.base_cidr_block
}


Argument vs attributes referrence
Arguments are defined as input variables iin resource or module's configuration. Where as attribute is used to retrieve info from resource that has already been created. it represents output of resource. 


-------------------Terraform CLI commands-------------------------
terraform version: find the terraform version
terraform -chdir=<path> <subcommand> : swicth to working directory

terraform init : initialize the directory for terraform configuration. mainly download required provider,modules any if referrenced in config  and install them locally to use them execute tf scripts --> creates lock file(terraform.locl.hcl) which records exact version of providers and modules installed --> creates .terraform dir where it installs providers and modules. 

terraform fmt : format the indentation of tf script.
terraform validate: validates the syntax 
terraform plan: create an execution plan (dry run)

terraform apply: when you run apply, terraform always run plan first then get the prompt to accept changes. once appoved then provision infra by applying changes. 
use -auto-approve to skip prompt (useful in CI/CD pipeline) --> updates state file to reflect new infra chnages + display output if output block defined. 

terraform destroy: destroy the created infra. 1st taraform locks project state (so that no other instance of terraform modify or apply changes to terraform resource) --> create plan to destroy resource and wait for approval --> once approved, destroy --> state file wil be updated with new state of resource

terraform show: shows attributes and properties of provisioned resources --> this output is read from state file. 

----------------Plan, Deploy and Cleanup commands------------------
terraform plan -out <plan_name> : output a deployment plan 
terraform plan destroy : output destroy plan 
terraform apply <plan_name>: apply specific plan 
terraform apply -taregt = <rsrc_name>: only apply changes to specific rsrc
terraform apply -var my_variable=<var_name>: pass var in command line
terraform providers: get providers info used in configuration



----Provider version contraints----- 
Used to set the accpetable range of versions for provider or module. If version is not specified then terraform downloads latest version 
Specify range of versions using various operators:
1. != excludes an exact version number
2. >=,<=,>,< greater,lower than equal to specified version
3. ~> only the rightmost version number(patch) to increase. (ex: 5.13.1  --> allowed 5.13.2 not 5.13.0)

manage terraform version:
Set the required_version = to control the version of terraform  that your config use and make updates more predictable. Avoid usiing floating versions ~>, which can lead to unexpected update. Version locking helps you to maintain consistency and predictability in your infra. 

-----Multiple provider configuration -----------
Primary reason for this is to support multiple regions for cloud platform. use alias meta-argument segament to provide an extra name segment. 
use <PROVIDER NAME>.alias for using it. 

terraform {
 required_providers{
  aws {
    source = "hashicorp/aws"
    version = "5.22.0"
   }
}

provider "aws"{
   region = 'ap-south-1"
   profile = "default"
  }

provider "aws"{
  region = "us-east-1"
  profile = "default"
  alias = "us-east-1"
 }

resource "aws_instance" "web"{
   region = aws
 }

region "aws_instance" "db"{
  region = us-east-1
 }


-----Local provider-------
Used to manage local resources such as creating files.

resource "local_file" "login"{
   content = "echo logging in"
   filename  = "./login.sh"
 }

resource "local_sensitive_file" "password"{
  content = "admin1233"
  filename = "./password.txt"
}

----Random provider------ 
The random provider allows the use of randomness within terraform configuration. This is terraform logical provider, works entirely within terraforms logic and does not interact with any other resource. 
(random_id, random_integer, random_pet, random_shuffle, random_string, random_uuid). ex: random_pet genrates random pet names which can be used as unique identifier with other resources. 

resource "random_pet" "serverName" {
  length = 3
  prefix = "ec2"
  seperator = "-"
 }

resource "aws_instance" "web"{
  ami = "ami-12sw12"
  instance_type = "t2.micro"
 
  tags  = {
    Name = random_pet.serverName.id
 }
}



---------------Input variables--------------------
Way to parameterize your config files. typically defined in separate variable files ( variables.tf) to keep code organized. terraform loads all files ending .tf s can name anything. 
variable block arguments:
description, type, default, sensitive (does not display but records in state file)

Data Types allowed:
string					list,object
number					set,tuple
bool					map

declaration:
variable "image_id"{
type = string
}

variable "availability_zone_names"{
type = list(string)
default=["us-west-1a"]
}

variable "docker_ports"{
type = list(object({
internal = number
external = number
protocol = string)})

default = [ {
internal = 8300
external = 8300
protocol = "tcp"
}]
}

variable "image_id"{					-- variable example with custom validation argument
type = string
description = "AMI"				
validation{
condition = length(var.image_id) > 4 && substr(var.image_id,0,4) == "ami-"
error_message = "image id must be valid"
}
}

supply variable values: 
1. variable default: specified in variable block (above)
2. command line flags: terraform apply -var="key=value" or -var='list=["ami1","ami2"]' or -var='map={key: value,key: value}'
3. File based variable: provided through separate file named terraform.tfvars and -var-file=variables.tfvars
resource "aws_instance" "web"{
 ami = var.ami
 instance_type = var.type
 
 tags  = {
   Name = var.name
 }
}

variables.tfvars
ami = "ami-0912f71e06545ad88"
type = "t2.micro"
name = "dev"


4. Environment variable: export TF_VAR_name=value

Precedence:
1. -var or -var-file       --> highest precendence 
2. *.auto.tfvars
3. terraform.tfvars.json
4. terraform.tfvars
5. Env variables

---Local variables----
like temp local variable defined within functions of pgrm language. set of local values can be declared together in local block and can be called using local.<NAME>
you can also define in separate file called locals.tf


-----------------------------Output Values---------------------------------------
they are like return values in prgm langauge and have many uses
1. child module can use them to expose subset of resource attributes to the parent module.
2. Root module can use them to print values in CLI.
3. Root module output can be accessed by other configurations via terraform-remote_state data source. 

can be declared using output block and file called output.tf
ex: 
resource "aws_instance" "server"{
 ami = "---"
 instance_type = "---"
}
output "instance_ip_addr"{
value = aws_instance.server.private_ip      -- expression result will be return to the user.
}


Arguments:
When accessing child module output in a parent module, the output of child module are available in expressions as 
module.<module_name>.<output_name>

description
sensitive		--Optional arguments for variable declaration
depends_on


#main.tf
module "foo"{
source = "./mod"
}
resource "test_instance" "x"{
some_attribute = module.mod.a
}
output "out"{
value = "xyz"
sensitive = true
}

output "a"{
value = "secret"
sensitive = true
}

--------depends_on argument example---------
output "instance_ip_addr"{
value = aws_instance.server.private_ip							-- depends_on should always be used as
description = "Private ip of server instance"						last resort. should always include 
											comment why it is being used.
depends_on=[
# security group rule must be created before this IP address actually be used. 
aws_security_group_rule.local_access
]
}


------------------------------------Data Sources-----------------------------------------------
In Terraform data sources are used to retrieve info from external sources and make it available for use in your infra code. Data sources are defined using data block
data "data_source_type" "data_source_name" {
 agrument = value 
}

ex: local_file,aws_caller_indentity,aws_ami(used to get ID of registered AMI for use in other rsrc) ,most_recent(use most recent one if terraform find match less than one) etc
data "aws_caller_identity" "current"{
 }

output "account_id"{
 value = data.aws_caller_identity.current.account_id
}
 
data "aws_key_pair" "existing_pair" {				--> use existing ec2 key pair to launch new instance. 
 key_name = "ansible"

}
resource "aws_instance" "web"{
 ami = "ami12321"
 instance_type = "t2.micro"
 key_pair = data.aws_key_pair.existing_pair.key_name
 tags  = {
  Name = "webserver"
 }
}

Filter function: used with data sources when you need to filter the results returned by data sources based on specific criteria. 
resource "aws_instance" "web"{
 ami = "ami12321"
 instance_type = "t2.micro"
 key_pair = data.aws_key_pair.existing_pair.key_name
 tags  = {
  Name = "Webserver"
 }
}

data "aws_instance""filtered_instance"{
 filter {
   name = "tag:Name"
   values = ["webserver"]
  }
 depends_on = [aws_instance.web]			--> depends_on meta argument used to define explicit dependancies between resources. 
}


data "aws_ami" "amazon_linux"{
  most_recent = true
  owners = ["amazon"]
 }
output "filtered_ami"{
 value = data.aws_ami.amazon_linux
}


------------------------------------Terraform provisioners---------------------------------------------------
Provisioners are used to execute script or actions on remote resources once they created. Provisioners are allow to perform configuration maagement tasks like installing software,running scripts etc
But provisioners are used as Last resort because they introduce complexity and uncertainty to terraform workflows as terraform runs the script but does not know what script does. 
Supports several types of provisioners:
1. File provisioners: Used to copy files or dir to newly created resource. require one or more connection block to access remote resource. 
resource "aws_instance" "web"{
  ami = "---"
  instance_type = "--"
  provisioner file {
     source = "./id_rsa.pub"
     destination = "/home/adm_swami/.ssh/id_rsa.pub"
  }
  
  connection {
    type = "ssh"
    user = "ansible"
    private_key = file("demo.key")  or password = "${var.password}"
    host = aws_instance.web.public_ip   (wrong)  			 --> expressions in connection block cannot refer to their parent resource by name. 
    host = self.public_ip							instead we use self keyword to refer to parents resource. 
  }
}

Note: connection block can be nested within either resource (affects all resource provisioners) or provisioner (affects only this provisioner) block

2. Local Exec provisioners: run scripts or commands on machine where Terraform is executed after resource is created. ex: running an ansible playbook
Failure Behavior:
by default, provisioners that wil fail also cause Terraform apply itself to fail. on_failure setting can be used to change this behaviour. 
coontiue: makes terraform to ignore and continue
fail: terraform will raise an error and stop applying. 

Creation time provisioners: only run during creation, not during update or any other lifecycle. if fails then resource is marked as tainted. tainted resource will be planned for destruction and recreation upon the next apply.  

destroy time provisioners: run before the resource is destroyed. If fail then terraform will error and rerun the provisioners again n next terraform apply. 
provisioner "local-exec"{
  when = create/destroy
  command = "echo 'Instance created/destroyed'"
  on_failure = continue/fail
 }

Null Resource: resource type that does not directly manage any infra. used for running local scripts, executing remote commands. provides a way to interact with external systems. 

resource "null_resource" "example"{
  triggers = {					--> triger block used to specify set of values that when changed will cause the null resource to be recreated. 
      timestamp = "$"{timestamp()}"
    }
  provisioner "local-exec" {
    command = "echo 'hello world'"
  }
}

3. Remote Exec provisioners: run scripts or commands on remote machine over SSH or WinRM
 resource "aws_instance" "web"{
  ami = "---"
  instance_type = "--"
  provisioner "remote_exec" {
     inline = [				--> each command is separated by ,
	"sudo yum update -y",
        "sudo yum install httpd -y",
        "sudo systemctl enable httpd --now"
     ]
  }
  
  connection {
    type = "ssh"
    user = "ansible"
    private_key = file("demo.key")  or password = "${var.password}"
    host = aws_instance.web.public_ip   (wrong)  			 --> expressions in connection block cannot refer to their parent resource by name. 
    host = self.public_ip							instead we use self keyword to refer to parents resource. 
  }
}



---------------------State Management-------------------------------
The Terraform state is necessary step for terraform to function, which maps real world infra to your config, keep track of your metadata(to create dependancy order for terraform apply and destroy operation) and to improve performance of large infra(Terraform stores a cache of attribute values for all resources in the state). 

When terraform creates a remote object, it will record identity of remote object in the state file.
Prior to any operation, Terraform does a refresh to update the state with real infra. 

ex: resource "aws_instance" "web" in config maps to the instance i-abcd1223 in the real infra. 
"refresh" step refers to the process of reconciling the Terraform state with the real-world resources in your infrastructure. 

Local Backend: Terraform stores the state file in current working directory where terraform runs. this is called local backend. 
Remote state backends(store state file remotely in s3): provide solution for collaboration, state locking, access control,  and data protection, making them a more reliable and secure option for managing infra. 
 
terraform state list 								--> list all the resources in current state file. 
terraform state show <resource_type>.<resource_name> 				--> views attribute and values of specific resource 
terraform state mv <resource_type>.<old_name> <resource_type>.<new_name>	--> change resource name without recreating it. 
terraform state rm <resource_type>.<resource_name>				--> Remove specified instance from state file. 
terraform state pull > new.tfstate 						--> manually pull remote state data from remote backend and save to file. 
terraform state push								--> push the state file to remote backend 
terraform state replace-provider <old_provider> <new_provider> 			--> change the provider config 


----Detecting and managing drift-----
Drift is the term for when the real-world state of your infra differes from the state defined in your config. drift can be introduced manually or any automation tool
so Terraform cannot detect drift of resources

Refresh-only mode:
terraform refresh (depricated)--> used to reconcile real world resources in your infra with terraform state. Its primarily used when you suspect that state has drifted out of sync with actual resources in infra. 

terraform plan -refresh-only or terraform apply -refresh-only


-----S3 as remote backend and DynamoDB to lock state file-----------
Aws S3 is also not immune to race condition (if multiple terraform run attempts to write to the same state file). S3 does not provide built in locking mechanisms for state file. 

terraform.lock.hcl 			-->  is primarily associated with managing module dependencies.
terraform.tfstate.lock.info 		--> is associated with the state locking mechanism when using remote backends.

Terraform will store the state within S3 and DynamoDB is used for state locking while performing changes. it ensures that only one user or process can update the state file at a time, preventing data corruption and race condition. 

1. Creat an S3 bucket
2. Create DynamoDB table.  --> Table must have partition key named LockID with type string. if not configured state locking will be disabled.  
3. Configure your Terraform Backend.  --> specify S3 bucket and DynamoDB table  as backend config using backend block

terraform {
  required_providers{
    aws {
       source = "hashicorp/aws"
       version = "5.22.0"
       }
    }
}

provider "aws"{
  region = "ap-south-1"
 }

backend "s3"{						--> can also be created using terraform		resource "aws_s3_bucket" "statefile_bucket"{
  bucket  = "statefile-2023"	or aws_s3_bucket.statefile_bucket						bucket = "statefile-2023"
  region = "ap-south-1"												
  key = "terraform.tfstate"											}
  dynamodb_table = "statelock-2023" or aws_dynamodb_table.statelock_table					resource "aws_dynamodb_table" "statelock_table"{
}														name = "statelock-2023"
														hash_key = "LockID"
resource "aws_instance" "web"{											attribute = {
 ami =  "ami-0912f71e06545ad88"												name = "LockID"
 instance_type = "t2.micro"												type = "S"
 															}
 tags = {													}
  Name = "WebServer"
  }
}


--------------------------Terraform workspace-----------------------
Terraform workspaces focus on managing different environments and configurations within a single codebase (single terraform configuration), with a primary emphasis on state isolation. (DRY approach)
Specially useful in scenarios where you have multiple deployment env. (dev,stage and prod etc)
terraform.tfstate.d/(dev/stage/prod)/terraform.tfstate			--> state isolation 

Terraform start with single,default workspace called default which cannot delete. 
terraform workspace new <name>				--> create workspace
terraform workspace list				--> list workspace
terraform workspace select <name>			--> select workspace
terraform workspace show				--> show current workspace
terraform workspace delete <name>			--> delete workspace. (should be in different workspace to delete)

#provider.tf
terraform {
  required_providers{
    aws{
        source = "hashicorp/aws"
        version = "5.22.0"
      }
  }
}

provider "aws"{
  region = "ap-south-1"
  profile = "default"
 }

#variable.tf
variable "instance_count"{
  type = map(number)
  default =  {
   "dev" = 2
   "stage" = 4
   "prod" = 4
 }
}
variable "ami" {
  type = map(string)
  default = {
   "dev" = "ami-0912f71e06545ad88"
   "stage" = "ami-0912f71e06545ad88"
   "prod" = "ami-0912f71e06545ad88"
  }
}

variable "instance_type"{
  type = map(string)
  default = {
   "dev" = "t2.micro"
   "stage" = "t2.medium"
   "prod" = "t2.large"
  }
}

#main.tf
resource "aws_instance" "web"{
  ami = lookup(var.ami,terraform.workspace)
  instance_type = lookup(var.instance_type,terraform.workspace)
  count = lookup(var.instance_count,terraform.workspace)

  tags = {
     Env = "${terraform.workspace}"
    }
}

#output.tf
output "instance_ip"{
 value = aws_instance.web[*].public_ip
 }



-------------------------------------------Modules----------------------------------
Terraform modules are powerful feature that helps in writing modular, maintainable and reusable infra code, making infra management more efficient and scalable. 
you can define set of resources, configurations, and variables within modules and then reuse that module in multiple terraform configurations. 
you can find prebuilt modules in terraform registry.


1. root module -- Every Terraform confguration has atleast one module, known as rot module, which consist of resources in .tf files in the main working directory. 
2. child module -- modules that are called by root module.
3. published module -- module that are loaded from public or private registry. (Terraform registry or GitHub)

project/			--> root module
|-- main.tf			
|-- variables.tf		 
|-- outputs.tf			
|-- terraform.tfvars		
|-- providers.tf		--> provider configuration can be defined only in root module. 
|-- modules/			--> child module
|   |-- networking/
|   |   |-- main.tf
|   |   |-- variables.tf
|   |   |-- outputs.tf		--> say it has output "instance_ip" { value = aws_instance.web.public_ip }. this is referred below
|   |-- compute/
|       |-- main.tf
|       |-- variables.tf
|       |-- outputs.tf
|-- .terraform/			
|-- terraform.tfstate		
|-- terraform.tfstate.backup


calling a child module:
module "local_name"{
source = "path to module"		--> "./modules/networking" from above structure
#arguments		
 providers = {
   aws = aws.eu2			--> ecu2 is alias of provider defined in provider.tf 	
}

Note: Providers configurations can be passed down to decendent modules 1. implicitly 2. explicitly via provider argument within module block. 

4 module argument types:
1. source argument is required for all modules.
2. verson argument is reccom for modules from public registry.
3. The input variable argument.
4. The meta argument like for_each and depends_on

example:						meta-arguments like
module "consul"{					1. count
source = "hashicorp/consul/aws"				2. for_each
version = 0.0.5"					3. depends_on
							4. providers
}

Module Source:
Module source tell terraform where to look for source code. terraform used the module source while module installaton
step of terraform init
There are 8 module source types:
1. local paths				5. generic git 
2. terraform registry			6. http urls
3. github				7. S3 bucket
4. bitbucket				8. GCS bucket

module "consul"{
source = "./consul"		--local module must start with ./ or ../
}

module "consul"{
source = "hashicorp/consul/aws"		--public registry
}

module "consul"{
source = "github.com/shantayya/example"		--github https url
}

module "consul"{
source = "git@github.com:shantayya/example.git"		-- github ssh url
}

module "consul"{
source = "https://example.com/vpc-module.zip"		-- fetching archives over http
}


Accessing module output values:
module.child_module_local_name.output_local_name			--> module.networking.instance_ip(for single ip) module.networking.public_ips (list)


#ec2/modules/create-ec2/main.tf
resource "aws_instance" "web"{
  ami = var.ami
  instance_type = var.instance_type
  count = var.instance_count
 }

#ec2/modules/create-ec2/variables.tf
variable "ami"{
  default = "ami-0912f71e06545ad88"
}

variable "instance_type"{
 default = "t2.micro"
}

variable "instance_count"{
 type = number
 default = 1
}


#ec2/modules/create-ec2/output.tf
output "public_ip"{
 value = aws_instance.web[*].public_ip
}


#ec2/provider.tf
terraform {
  required_providers {
    aws {
      source = "hashicorp/aws"
      version = "5.22.0"
     }
   }
 }

provider "aws"{
   region = "ap-south-1"
}


#ec2/main.tf
module "ec2"{
  source = "./modules/create-ec2"
  instance_count = 2				--> to override count from child module
}

#ec2/output.tf
output "ip"{
  value = module.ec2.public_ips			--> public_ips output in the create-ec2 module is a list of public IP addresses from the AWS instances.
}							You can then reference this list in the root module as module.ec2.public_ips.


------Writing custom terraform module and publish to registry------
1. module must be on GitHuba and must be public repo
2. Name of repo should be three part frmat like terraform-<PROVIDER>-name. name segment can have multiple highen like terraform-aws-mutli-ec2
3. there should be release tag created in semantic versioning format ex: v1.0.0
4. Once a tag is available in the repo, goto terraform registry and sign in with GitHub
5. Regisry will automatically scan the repo with all the valid naming conventions and tags for publishing the modules. 

git commands to tag:
git tag -a v1.0.0 - m "frst tag"
git push origin v1.0.0

---------------------------------------Funtions--------------------------------------------------
Element function: retrieves a single element from list. 
syntax: element(list,index)

file function: reads the content of file at given path and returns them as string. 
resource "aws_key_pair" "demo"{
  key_name = "ansible"
  public_ey = file("demo_key.pub")
}

resource "aws_instance" "web"{
  key_name = aws_key_pair.demo.key_name
  ami = "---"
  instance_type = "--"
}


-----------------------Terraform Imports------------------------------------------------
used to import existing infra resource into your terraform state,allowing you to manage them as part of infra. 
terraform import <RESOURCE_TYPE>.<RESOURCE_NAME> ADDRESS
		 aws_instance    web		 unique identifier

say, we have ec2 instance (web) running. 
Before importing the resource, the root module should have the provider downloaded and the resource block with resource type being imported. 
terraform {
  required_providers{
    aws {
       source = "hashicorp/aws"
       version = "5.22.0"
       }
  }

provider "aws"{
   region = var.region

resource "aws_instance" "web"{
}

execute: terraform import aws_instance.web instance-id			--> it will import already running web instance into infra and update state file. 
									 but config file will not be updated. 

terraform state show aws_instance.web > ec2.tf				--> but not effective as it will have unsupported attributes which are not arguments


-------------------------Terraform Taint----------------------------------
taint is used to mark resource for recreation. untaint command removes the tainted status from resource, making it clean. the resurce will not be replaced during next tarrform apply

terraform taint RESOURCE_TYPE.RESOURCE_NAME			--> depricated command
terraform untaint RESOURCE_TYPE.RESOURCE_NAME

terraform apply -replace=RESOURCE_TYPE.RESOURCE_NAME

we can also taint resource insie module:
terraform taint module.module_local_name.resource_name


---------------------------------------Meta Arguments-------------------------------------
Special argument that control how resources are created,updated and destroyed. They provide additional configuration options beyond regular resoruce specific arguments 
Meta-argument can be used with resource and module block. 

depends_on: 	Specify hidden dependancies
module "vpc"{
  source = "./modules/vpc"
}

module "eks"{
  source = "./modules/eks"
  depends_on = [module.vpc]
}

count:		By default terraform creates one resource per resource block. count multiple resource instances according to count
for_each: 	create multiple resources according to map or set of strings. object has two attributes
	each.key --> map key (or set member) corresponding to this instance
	each.value --> map value corresponding to this instance. (if set then same as key)

	variable "resource_types"{
           type = map(string)
           default = {
              "master" = "t2.micro"
              "web-01" = "t2.medium"
              "web-02" = "t2.medium"
             }
          }
         resource "aws_instance" "web"{
            for_each = var.instance_types
            ami = "ami-0912f71e06545ad88"
            instance_type = each.value
            tags = {
              	Name = "k8s-${each.key}"
                }
            }

provider: 	select non-deafult provider configuration 
lifecycle: 	set lifecycle customization. include options like (bool value) create_before_destroy,prevent_destroy,ignore_changes,replace_triggered_by etc. its nested block inside respource. 
 lifecycle{
        create_before_destroy = true
   }

provisioner and connections:  take extra action after resource creation. (bootstrap script)


--------------------------------Resource targeting-----------------------
By Default terraform will apply entire plan at once. In some situation you may want to apply changes to specific resource or modules within your terraform config. this can be helpful for troublshooting issue or dealing with cases where terraform state becomes out of sync with actual resource. To support this terraform lets you target specific resources when you plan,apply and destroy infra. 

terraform apply -target=resource_type.resource_names


Note: The -target option is not explicitly designed for addressing drift; it's more for selectively applying changes to specific resources. If you're using it during a drift scenario, it's important to understand that it won't automatically resolve drift issues. 


-----------------------------Dynamic block--------------------------------------
Dynamic blocks provide a way to construct repetative nested blocks dynamically within a resource config. can be defined inside resource,data,provider and provisioner blocks. other ways to acheive same task through count,for_each 

variable "inbound_ports"{
  type = list(number)
  default = [22,80,44]
}

resource "aws_security_group" "sg"{
  name = "test_sg"
  
  dynamic "ingress"{
     for_each = var.unbound_ports
     content {
        description = "allow traffic on ${iingress.value}"
        from_port = ingress.value					--> here it should be dynamic block name.value not each.value
        to_port = ingress.value
        protocol = "tcp"
        cidr_block = "0.0.0.0/0"
        ipv6_cidr_block = "[::/0]"
     }
 }
 tags = {
    env = "prod"
  }
}
        

--------------Directories and Files-----------------
1. File extention: code in terrform language is stored in .tf or .tf.json file extention. 
2. Directories and Modules: Modules are collection of .tf or .tf.json files kept together in directory.
module consist of only top level config files in directory. nested directory is treated as separate module and may not be automatically included.
3. Root module: current woking directory where tarrform is invoked. 

-----------Override files-----------
using override.tf 

resource "aws_instance" "web"{
instance_type = "t2.micro"		: terraform config example
ami = "ami-408c7f28"
}


resource "aws_instance" "web"{
ami = "foo"				: override.tf
}


resource "aws_instance" "web"{		: This is how the merge is treated
instance_type = "t2.micro"
ami = "foo"
}

# or //: single line comment 
/* */ : multiline comment


-----Operation timeouts--------------
resource "aws_instance" "web"{
timeouts{
create="60m"				timeout string example:
delete="2h"					"60m" "10s" "1h"
}
}
There are some resource types that provide special timeouts,nested block arguments that allow for customization of how long
certain operations are allowed to take before they are deemed failed. 

----How configuration is applied---------
1. create: create resources that exist in the configuration but not associated with real infra objects in state.
2. destroy: destroy resource exist in the state but no longer exist in the configuration.
3. update in-place: Update in-place resources whose arguments have changed. 
4. Destroy and re-create: Destroy and re-create resources whose arguments have changed But which cannot be updated in-place
due to remote api limitations. 



----How to assign values to root module variables----
1. In terraform cloud workspace
2. -var command line option  (last priority)
3. in variable definitions like .tfvars or .tfvars.json file   (2nd priority)
4. env variable like export TF_VAR_ followed by name of variable   (1st priority of variable to be loaded)


condition ? true_val : false_val		- conditional expression
var.example ? 12 : "hello"

----working with backend------------
each terraform configuration can specify a backend.
1. terraform begginers -- local backend
2. if working with teams and large infra then use remote backend

Two areas of terraform behavior are determined by backend:
1. where the state is stored
2. where the operations are performed. 

------------------------------------------------Terraform scripts------------------------------------
#Provision AWS ec2 using terraform
1. Terraform CLI installed
2. AWS CLI installed and configured (to use IAM crdentials to auth terraform aws provider)

terraform {
  required_providers{
   aws{
       source = "hashicorp/aws" 	
       version = "5.22.0"			--> aws provider version	
      }
required_version = ">0.13.0"			--> terraform CLI version	
}
   provider  "aws" {
      region = "ap-south-1"
      }
 
resource "aws_instance" "web"{
   ami = "ami12123"
   instance_type = "t2.micro"
   
   tags = {
     Name = "Dev"
     }
 }


---------------------Create mysql rds db using terraform------------
#provider.tf 
terraform {
  required_providers{
    aws {
       source = "hashicorp/aws"
       version = "5.22.0"
       }
  }

provider "aws"{
   region = var.region
   profile = var.default
 }

#variable.tf
variable "region" {
  default = "ap-south-1"
}

variable "storage"{
 type = number
 default = 2
}

variable "username"{
 default = "admin"
}

variable "password"{
 sensitive = true
}

#main.tf
resource "aws_db_instance" "mydb"{
  allocated_storage = var.storage
  db_name = "mydb"
  engine = "mysql"
  engine_version = "5.7"
  instance_class = "db.t3.micro"
  username = var.username
  password = var.password
  parameter_group_name = "default.mysql5.7" 
  skip_final_snapshot = true
  publically_accessible = true
}

#output.tf
output "address"{
 value = aws_db_instance.mydb.address
}

output "endpoint"{
 value = aws_db_instance.mydb.endpoint
}


------------------create security group ---------------------
resource "aws_security_group" "allow_tls"{
  name = "allow_tls"
  description = "allow traffic on port 22 and port 80"
  ingress {
    description = "allow traffic on port 22"
    from_port = 22
    to_port = 22
    protocol = "tcp" 
    cidr_range = [0.0.0.0/0]
    ipv6_cidr_range = [::/0]
   }
  ingress {
    
    description = "allow traffic on port 80"
    from_port = 80
    to_port = 80
    protocol = "tcp" 
    cidr_range = [0.0.0.0/0]
    ipv6_cidr_range = [::/0]
 }

egress {
   description  = "allow all outgoing traffic"
   from_port = 0
   to_port = 0
   protocol = "-1"
   cidr_range = [0.0.0.0/0]
   ipv6_cidr_range = [::/0]
}

 tags = {
   Name = "sg"
  }
}

resource "aws_instance" "web"{
   ami = "ami12123"
   instance_type = "t2.micro"
   security_groups = [aws_security_group.allow_tls]
   tags = {
     Name = "Dev"
     }
   depends_on = [aws_security_group.allow_tls]
 }


-------------AWS user data through file function and variables----
When user data script is processed it is copied to  and run from /var/lib/cloud/instances/instance-id/
The log of userdata are stored in /var/log/cloud-init-output.log

variable "user_data"{
 type = string
 default = <<- EOF
   sudo yum update 
   sudo yum install httpd
   sudo systemctl enable httpd --now
 EOF
 }

#script.sh
sudo yum update 
sudo yum install httpd
sudo systemctl enable httpd --now

resource "aws_instance" "web"{
 ami = var.ami
 instance_type = var.instance_type
 key_name = aws_key_pair.demo.key_name
 security_groups = [aws_security_group.allow_tls]
 
 user_date = file("script.sh") or var.user_data
 tags = {
   Name = "WebServer"
  }
}


---------------------Using null resource and local exec provisioner to create ansible host file----------
variable.tf
variable "instance_count"{
  type = number 
  default = 3
 }

variable "ami"{
  ami = "ami-0912f71e06545ad88"
}

variable "instance_type"{
 default = "t2.micro"
}


resource "aws_instance" "web"{
  ami = var.ami
  instance_type = var.instance_type
  count = var.instance_count

  tags = {
    Name = "WebServer"
   }
  
#  provisioner "local-exec" {					--> local-exec provisioner will execute every time new instance created but what if instance already       #	command = "echo ${self.public_ip} >> ./hosts"			created? create null resource and execute loca provisioner then 
  }	
}


resource "null_resource" "print_ip" {
  count = length(aws_instance.web)
  
  provisioner "local-exec" {
    command = "echo ${element(aws_instance.web[*].public_ip,count.index)} >> ./hosts"
  }
}


--------------Terraform Ansible integration----------------------
1. confiure AWS CLI and iam user with admin permissions
2. install ansible and terraform

#install_http.yaml
become: true
become_user: ec2-user
tasks:
  - name: install httpd
    yum:
     name: httpd
     state: latest
    notify: start and enable httpd
  - name: copy web page
    copy:
     src: index.html
     dest: /var/www/html/index.html
handlers:
  - name: start and enable httpd
    service:
     name: httpd
     state: started
     enabled: yes

variable.tf
variable "instance_count"{
  type = number
  default = 3
}

resource "aws_instance" "web"{
 ami = "ami-0912f71e06545ad88"
 instance_type = "t2.micro"
 count = var.instance_count
 
 tags = {
   Name = "WebServer-${count.index}"
 }

 provisioner "local-exec" {
   command = "echo ${element(aws_instance.web[*].pubic_ip,count.index)} >> ./hosts"
  }

 provisioner "local-exec" {
   command = "ansible-playbook -i hosts install_http.yaml"   or "ansible-playbook -i '${self.public_ip,}' install_http.yaml"
 }

}

output "access_url"{
  value = "http://${aws_instance.web.public_ip}"
}



-------------creating AWS VPC using vpc module----------
#vpc/variables.tf
variable "region"{
  default = "ap-south-1"
 }

variable "vpc_name"{
  default = "my-vpc"
}

variable "cidr"{
 default = "10.0.0.0/16"
}

variable "public_subnet"{
 type = list(string)
 default = ["10.0.1.0/24","10.0.2.0/24","10.0.3.0/24"]
}

variable "private_subnet"{
 type = list(string)
 default = ["10.0.101.0/24","10.0.102.0/24","10.0.103.0/24"]
}

#vpc/provider.tf
terraform {
  required_providers{
    aws {
       source = "hashicorp/aws"
       version = "5.22.0"
       }
  }

provider "aws"{
   region = var.region
}


#vpc/main.tf
module "vpc"{
  source = "aws-modules-vpc/aws/vpc"
  version = "5.1.2"
  
  name = var.vpc_name
  azs = data.aws_availability_zones.available.names
  private_subnet = var.private_subnet
  public_subnet = var.public_subnet

  enable_nat_gateway = true
  single_nat_gateway = true
  one_nat_gateway_per_az = false
  map_public_ip_on_launch = true
  
  public_subnet_tags = {
    subnet = "public"
    "kubernetes.io/role/elb" = "1"
  }

  private_subnet_tags = {
    subnet = "private"
    "kubernetes.io/role/internal-elb" = "1"
  }

  tags = {
    Env = "dev"
   }
}
  
  data "aws_availability_zones" "available"{
    state = "available"
  }


#vpc/output.tf
output "public_subnet_id"{
  value = module.vpc.public_subnets
}

output "private_subnet_id"{
 value = module.vpc.private_subnets
 }

output "igw"{
  value = module.vpc.igw_id    
}



--------------------Create AWS EKS and VPC using terraform -------------------------
#vpc.tf
resource "aws_vpc" "my-vpc"{
  name = "eks-vpc"
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "eks-vpc"
   }
}

resource "aws_internet_gateway" "igw"{
  vpc_id = aws_vpc.my-vpc.id
  tags = {
    Name = "igw"
  }
}

#subnet.tf
resource "aws_subnet" "private_01"{
  vpc_id = aws_vpc.my-vpc.id
  cidr_block = "10.0.1.0/24"
  tags = {
    subnet = "private-01"
    "kubernetes.io/role/internal-elb" = "1"
   }
}

resource "aws_subnet" "private_02"{
  vpc_id = aws_vpc.my-vpc.id
  cidr_block = "10.0.2.0/24"
  tags = {
    subnet = "private-02"
     "kubernetes.io/role/internal-elb" = "1"
   }
}

resource "aws_subnet" "public_01"{
  vpc_id = aws_vpc.my-vpc.id
  cidr_block = "10.0.3.0/24"
  tags = {
    subnet = "public-01"
     "kubernetes.io/role/elb" = "1"
   }
}

resource "aws_subnet" "public_02"{
  vpc_id = aws_vpc.my-vpc.id
  cidr_block = "10.0.4.0/24"
  tags = {
    subnet = "public-02"
     "kubernetes.io/role/elb" = "1"
   }
}

#nat.tf
resource "aws_eip" "eip"{
  tags = {
   Name = "eip"
  }
}

resource "aws_nat_gateway" "nat_gw"{
  allocation_id = aws_eip.eip.id
  subnet_id = aws_subnet.public_01.id
  tags = {
    Name = "nat-gw"
  }
 depends_on = [aws_internet_gateway.igw]
}



#routes.tf
resource "aws_routes_table" "public_rt"{
 vpc_id = aws_vpc.my-vpc.id
 route {
    cidr_block = "0.0.0.0/24"
    gateway_id = aws_internet_gateway.igw.id
    }
}

resource "aws_routes_table" "private_rt"{
 vpc_id = aws_vpc.my-vpc.id
 route {
    cidr_block = "0.0.0.0/24"
    gateway_id = aws_nat_gateway.nat_gw.id
    }
}

resource "aws_route_table_association" "public_subnet_01"{
  subnet_id = aws_subnet_public_01.id
  route_table_id = aws_route_table.public_rt.id
}

resource "aws_route_table_association" "public_subnet_02"{
  subnet_id = aws_subnet_public_02.id
  route_table_id = aws_route_table.public_rt.id
}

resource "aws_route_table_association" "private_subnet_01"{
  subnet_id = aws_subnet_private_01.id
  route_table_id = aws_route_table.private_rt.id
}

resource "aws_route_table_association" "private_subnet_02"{
  subnet_id = aws_subnet_private_02.id
  route_table_id = aws_route_table.private_rt.id
}


#cluster-role.tf
resource "aws_iam_role" "cluster_role"{
 name = "test-role"
 assume_role_policy = jsonencode({
    "Version": "2012-10-17"
    "Statement": [
        {
          "Effect":"Allow",
          "Principal":{
             "Service": "eks.amazon.com"
              },
          "Action": "sts:AssumeRole"
        }
      ]
     })

 tags  = {
    Name = "cluster-role"
   }
}

resource "aws_iam_role_policy_attachment" "eks_managed_policy"{
   role = aws_iam_role.cluster_role.name
   policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

#eks_cluster.tf
resource "aws_eks_cluster" "demo_cluster"{
  name = var.cluster_name
  role_arn = aws_iam_role.cluster_role.arn
 
  vpc_config{
    subnet_ids = [aws_subnet.private_01.id,aws_subnet.private_02.id,aws_subnet.public_01.id, aws_subnet.public_02.id]
   }

 depends_on = [aws_iam_role_polic_attachment.eks_managed_policy]
 }

output "endpoint"{
  value = aws_eks_cluster.demo_cluster.endpoint
 }


#node_role.tf
resource "aws_iam_role" "node_role"{
 name = "node-role"
 assume_role_policy = jsonencode({
    "Version": "2012-10-17"
    "Statement": [
        {
          "Effect":"Allow",
          "Principal":{
             "Service": "eks.amazonaws.com"
              },
          "Action": "sts:AssumeRole"
        }
      ]
     })

 tags  = {
    Name = "node-role"
   }
}

resource "aws_iam_role_policy_attachment" "eks_node_policy"{
   role = aws_iam_role.node_role.name
   policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}
resource "aws_iam_role_policy_attachment" "eks_ecr_read_policy"{
   role = aws_iam_role.node_role.name
   policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}
resource "aws_iam_role_policy_attachment" "eks_cni_policy"{
   role = aws_iam_role.node_role.name
   policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

#managed_nodegroup.tf
resource "aws_eks_node_group" "managed_node_group" {
  cluster_name    = aws_eks_cluster.demo_cluster.name
  node_group_name = "managed_node_group"
  node_role_arn   = aws_iam_role.node_role.arn
  subnet_ids      = [aws_subnet.private_01.id,aws_subnet.private_02.id]

  scaling_config {
    desired_size = 2
    max_size     = 5
    min_size     = 2
  }

  update_config {
    max_unavailable = 1
  }

  # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling.
  # Otherwise, EKS will not be able to properly delete EC2 Instances and Elastic Network Interfaces.
  depends_on = [
    aws_iam_role_policy_attachment.eks_node_policy,
    aws_iam_role_policy_attachment.eks_ecr_read_policy,
    aws_iam_role_policy_attachment.eks_cni_policy
  ]
}

#variables.tf
variable "region" {
  default = "ap-south-1"
}
variable "cluster_name"{
 default = "my-test-cluster"
}

#update-kubeconfig.tf
resource "null_resource" "update_kubeconfig"{
  depends_on = [aws_eks_cluster.demo_cluster]
  provisioner "local-exec"{
  command = "aws eks update-kubeconfig --region ${var.region} --name ${var.cluster_name}"
  when = create
  }
}


#deployment.yaml
apiVersion: v1
kind: Deployment
metadata:
  app: connected-city
spec:
  replicas: 3
  selector:
    matchLabels:
      app : connected-city
  template:
    metadata:
     labels:
       app : connected-city
    spec: 
      containers:
      - name: connected-city
        image: shantayyaswami/connected-city:v1
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  app: connected-city
spec:
  selector:
    matchLabels:
     app: connected-city
  ports:
  - protocol: tcp
    port: 80
    targetPort: 5000
  type: LoadBalancer
   

-------------------------------------------Jenkins-Terraform intergation-------------------------------------

1. create k8s cluster on digital ocean and downlaod kubeconfig file locally
2. download helm and kubectl and set path 
3. create jenkins-terraform user wth admin permissions and download AWS_ACCESS_KEY and AWS_SECRET_KEY
4. install jenkins through helm chart
helm repo add jenkins https://charts.jenkins.io
helm repo update
helm pull jenkins/jenkins --untar			

helm upgrade --install jenkins jenkins/ 					--> you have to make below changes in values.yaml file and then execute
or 
helm upgrade --install jenkins jenkins/jenkins --set controller.servicePort=80 --set controller.serviceType=LoadBalancer		--> changing service type to loadbalancer so that jenkins can be directly accessible from outside cluster

kubectl exec -it jenkins-0 -- bash
cat /run/secrets/additional/chart-admin-password

pipeline{
  agent{
    kubernetes{
      defaultContainer 'terraform'	
      yaml '''
      apiVersion: v1
      kind: Pod
      metadata:
        labels:
          name: IaC
      spec:
        containers:
        - name: terraform
          image: hashicorp/terraform
          command:
          - cat
          tty: true
        - name: SCM
          image: git:alpine
          command: 
          - cat
          tty: true
       '''}
     }
   parameters{ choice(name: 'WORKFLOW',choices: ['apply','destroy'],description: 'Chose action')}
    stages{
        stage('Checkout SCM'){
            steps{
                container('git'){
                    sh 'git clone URL'
                              }
                   }
                }
         stage('Terrafrom init'){
             steps{
                 container('terraform'){
                    	withCredentials([usernamePassword(credentialsId:'terraform', passwordVariable:'AWS_ACCESS_KEY', userVariable:'AWS_SECRET_KEY')]){
					sh 'terraform init'
				}
                             }
                    }
              }
	  stage('Terrafrom ${WORKFLOW}'){
             steps{
                 container('terraform'){
                    	withCredentials([usernamePassword(credentialsId:'terraform', passwordVariable:'AWS_ACCESS_KEY', userVariable:'AWS_SECRET_KEY')]){
					sh 'terraform ${WORKFLOW} -auto-approve'
				}
                             }
                    }
              }
         }
  }

Note: make sure that remote backend is created. 

will above integration work?

